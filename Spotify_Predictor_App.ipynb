{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ddc6abf",
   "metadata": {},
   "source": [
    "# House Tyrell's Spotify Genre, Decade, and Song Recommender\n",
    "This app predicts genre and decade for a given song/track and artist using Random Forest Classifier models.    \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a0582ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.display import display, clear_output, Markdown, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "#Load the Trained Model and the fit Scaler\n",
    "rfModel_decade = joblib.load('rfModel.joblib')\n",
    "scaler_decade = joblib.load('scaler.joblib')\n",
    "\n",
    "rfModel_genre = joblib.load('rfModel_genre.joblib')\n",
    "scaler_genre = joblib.load('scaler_genre.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3730bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6293d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your Spotify API credentials\n",
    "client_id = '7f37bb631ed34cf29a487f88d5c5e32f'\n",
    "client_secret = 'aff5f020a49b470f99d7a3c251f5b649'\n",
    "\n",
    "# Authenticate with the Spotify API\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "# Define a function to get track audio features\n",
    "def get_track_audio_features(artist_name, track_name):\n",
    "    # Search for the track\n",
    "    results = sp.search(q=f\"artist:{artist_name} track:{track_name}\", type='track')\n",
    "\n",
    "    # Check if the search results contain any tracks\n",
    "    if len(results['tracks']['items']) == 0:\n",
    "        print(f\"No track found for '{artist_name} - {track_name}'\")\n",
    "        return None\n",
    "\n",
    "    # Get the first track from the search results\n",
    "    track = results['tracks']['items'][0]\n",
    "\n",
    "    # Get the track's ID\n",
    "    track_id = track['id']\n",
    "\n",
    "    # Get audio features for the track\n",
    "    audio_features = sp.audio_features(track_id)\n",
    "\n",
    "    # Extract desired features\n",
    "    track_features = {}\n",
    "#    track_features['Artist'] = artist_name\n",
    "#    track_features['Track'] = track_name\n",
    "    track_features['Valence'] = audio_features[0]['valence']\n",
    "    track_features['Acousticness'] = audio_features[0]['acousticness']\n",
    "    track_features['Danceability'] = audio_features[0]['danceability']\n",
    "    track_features['Duration (ms)'] = audio_features[0]['duration_ms']\n",
    "    track_features['Energy'] = audio_features[0]['energy']\n",
    "    track_features['Explicit'] = track['explicit']\n",
    "    track_features['Instrumentalness'] = audio_features[0]['instrumentalness']\n",
    "    track_features['Key'] = audio_features[0]['key']\n",
    "    track_features['Liveness'] = audio_features[0]['liveness']\n",
    "    track_features['Loudness'] = audio_features[0]['loudness']\n",
    "    track_features['Mode'] = audio_features[0]['mode']\n",
    "    track_features['Popularity'] = track['popularity']\n",
    "    track_features['Speechiness'] = audio_features[0]['speechiness']\n",
    "    track_features['Tempo'] = audio_features[0]['tempo']\n",
    "    track_features['Time Signature'] = audio_features[0]['time_signature']\n",
    "    \n",
    "    # Get the release date of the track\n",
    "    release_date = track['album']['release_date']\n",
    "    actual_year = release_date.split('-')[0]       # Extract the year from the release date\n",
    "    \n",
    "    # Get the spotify song recomendations\n",
    "    recommendations = sp.recommendations(seed_tracks=[track_id], limit=5)\n",
    "    \n",
    "    # Get the track URL\n",
    "    track_url = track['external_urls']['spotify']\n",
    "    \n",
    "    # Get the album cover images and URL of the first cover image\n",
    "    album_cover_images = track['album']['images']\n",
    "    cover_image_url = album_cover_images[0]['url']\n",
    "\n",
    "    return track_features, actual_year, recommendations, track_url, cover_image_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5284617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_decade(track_features): \n",
    "    # Create a new dictionary to store a decade-model-specific dictionary\n",
    "    track_features_decade = {key: value for key, value in track_features.items() if key != 'Time Signature'}\n",
    "    \n",
    "    # Convert the dictionary to an array\n",
    "    feature_array_decade = [track_features_decade[key] for key in track_features_decade]\n",
    "    \n",
    "    feature_array_decade = np.array(feature_array_decade).reshape(1, -1)\n",
    "    spotify_track_scaled_decade = scaler_decade.transform(feature_array_decade)  # Preprocess the sample data using the scaler\n",
    "\n",
    "    # Make the prediction\n",
    "    predicted_decade = rfModel_decade.predict(spotify_track_scaled_decade)\n",
    "    \n",
    "    return predicted_decade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "182e3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_genre(track_features):     \n",
    "    # Rearrange track_features to match the genre rfModel\n",
    "    track_features_genre = {}\n",
    "    track_features_genre['Popularity'] = track_features['Popularity']\n",
    "    track_features_genre['Danceability'] = track_features['Danceability']\n",
    "    track_features_genre['Energy'] = track_features['Energy']\n",
    "    track_features_genre['Key'] = track_features['Key']\n",
    "    track_features_genre['Loudness'] = track_features['Loudness']\n",
    "    track_features_genre['Mode'] = track_features['Mode']\n",
    "    track_features_genre['Speechiness'] = track_features['Speechiness']\n",
    "    track_features_genre['Acousticness'] = track_features['Acousticness']\n",
    "    track_features_genre['Instrumentalness'] = track_features['Instrumentalness']\n",
    "    track_features_genre['Liveness'] = track_features['Liveness']\n",
    "    track_features_genre['Valence'] = track_features['Valence']\n",
    "    track_features_genre['Tempo'] = track_features['Tempo']\n",
    "    track_features_genre['Duration (ms)'] = track_features['Duration (ms)']\n",
    "    track_features_genre['Time Signature'] = track_features['Time Signature']\n",
    "    \n",
    "    # Convert the dictionary to an array\n",
    "    feature_array_genre = [track_features_genre[key] for key in track_features_genre]\n",
    "    \n",
    "    feature_array_genre = np.array(feature_array_genre).reshape(1, -1)\n",
    "    spotify_track_scaled_genre = scaler_genre.transform(feature_array_genre)  # Preprocess the sample data using the scaler\n",
    "\n",
    "    # Make the prediction\n",
    "    prediction = rfModel_genre.predict(spotify_track_scaled_genre)\n",
    "\n",
    "    # Convert prediction output to Genre\n",
    "    genre_label = {\n",
    "        0: 'Acoustic/Folk',\n",
    "        1: 'Alt_Music',\n",
    "        2: 'Blues',\n",
    "        3: 'Bollywood',\n",
    "        4: 'Country',\n",
    "        5: 'HipHop',\n",
    "        6: 'Indie Alt',\n",
    "        7: 'Instrumental',\n",
    "        8: 'Metal',\n",
    "        9: 'Pop',\n",
    "        10: 'Rock'\n",
    "    }\n",
    "\n",
    "    predicted_genre = genre_label[prediction[0]]\n",
    "    \n",
    "    return predicted_genre\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19d60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_track_spider_data(artist_name, track_name):\n",
    "    track_features, actual_year, recommendations, track_url, cover_image_url = get_track_audio_features(artist_name, track_name)\n",
    "\n",
    "    if track_features is None:\n",
    "        return None\n",
    "\n",
    "    # Extract categories and values from track features\n",
    "    categories = ['Valence', 'Acousticness', 'Danceability', 'Energy', 'Instrumentalness',\n",
    "                  'Liveness', 'Loudness', 'Speechiness', \n",
    "                  'Key', 'Tempo', 'Duration (min)', 'Popularity', 'Mode', 'Time Signature']\n",
    "    values = [track_features['Valence']*10, track_features['Acousticness']*10, track_features['Danceability']*10,\n",
    "              track_features['Energy']*10, track_features['Instrumentalness']*10, track_features['Liveness']*10,\n",
    "              track_features['Loudness']/-10, track_features['Speechiness']*10, \n",
    "              track_features['Key'], track_features['Tempo']/10, track_features['Duration (ms)']/1000/60, \n",
    "              track_features['Popularity']/10, track_features['Mode']*10, track_features['Time Signature']]\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Append the starting angle to close the plot.\n",
    "    \n",
    "    fig, ax = plt.subplots(subplot_kw={'polar': True})\n",
    "    \n",
    "    ax.plot(angles, values + values[:1], linewidth=1, linestyle='solid', marker='o')\n",
    "    ax.fill(angles, values + values[:1], alpha=0.25)\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories)\n",
    "    \n",
    "#    plt.title('Song Features')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2873407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HTML Markdown to style the headers\n",
    "def styled_header(header_text):\n",
    "    styled_header = '''\n",
    "    <div style=\"background-color: gray; padding: 6px; font-family: Arial; text-align: left;\">\n",
    "        <h2 style=\"color: white; text-decoration: underline;\">{}</h2>\n",
    "    </div>\n",
    "    '''.format(header_text)\n",
    "    return styled_header\n",
    "\n",
    "def styled_paragraph(paragraph_text):\n",
    "    styled_paragraph = '''\n",
    "    <div style=\"font-family: Arial; color: blue; font-style: italic; font-size: 20px;\">\n",
    "        <p>{}</p>\n",
    "    </div>\n",
    "    '''.format(paragraph_text)\n",
    "    return styled_paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353f0151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797ee71c58e04671aa81f0a5b0c600ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Track Name:'), Text(value='', description='Artist Name:'), Button(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create text input widgets\n",
    "track_name_widget = widgets.Text(description='Track Name:')\n",
    "artist_name_widget = widgets.Text(description='Artist Name:')\n",
    "\n",
    "# Create button widget\n",
    "button = widgets.Button(description='FIND MY SONG!')\n",
    "\n",
    "# Create image widget\n",
    "image_widget = widgets.Image()\n",
    "\n",
    "# Modify the button style\n",
    "button.style.button_color = 'lightblue'\n",
    "button.layout.width = '200px'\n",
    "button.layout.height = '50px'\n",
    "button.layout.font_size = '20px'\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define function to handle button click event\n",
    "def on_button_click(b):\n",
    "    track_name = track_name_widget.value\n",
    "    artist_name = artist_name_widget.value       \n",
    "    \n",
    "    with output:\n",
    "        clear_output(wait=True) \n",
    "        \n",
    "        track_features, actual_year, recommendations, track_url, cover_image_url = get_track_audio_features(artist_name, track_name)\n",
    "        predicted_decade = predict_decade(track_features)\n",
    "        predicted_genre = predict_genre(track_features)\n",
    "        \n",
    "        print(\"\")\n",
    "        \n",
    "        audio_element = f'<audio src=\"{track_url}\" controls></audio>'\n",
    "        display(HTML(audio_element))\n",
    "        \n",
    "        display(Markdown('---'))  # Display horizontal line\n",
    "        predicted_decade_header = styled_header(\"Predicted Decade (using House Tyrell ML Engine)\")\n",
    "        predicted_decade_line1 = styled_paragraph(f\"Predicted decade for '{track_name}' by {artist_name}: {predicted_decade[0]}\")\n",
    "        predicted_decade_line2 = styled_paragraph(f\"Actual year for '{track_name}' by {artist_name}: {actual_year}\")\n",
    "        \n",
    "        display(HTML(predicted_decade_header))\n",
    "        display(HTML(predicted_decade_line1))\n",
    "        display(HTML(predicted_decade_line2))\n",
    "        \n",
    "        display(Markdown('---'))  # Display horizontal line\n",
    "        predicted_genre_header = styled_header(\"Predicted Genre (using House Tyrell ML Engine)\")\n",
    "        predicted_genre_line = styled_paragraph(f\"Predicted genre for '{track_name}' by {artist_name}: {predicted_genre}\")\n",
    "        \n",
    "        display(HTML(predicted_genre_header))\n",
    "        display(HTML(predicted_genre_line))        \n",
    "        \n",
    "        display(Markdown('---'))  # Display horizontal line \n",
    "        song_recs_header = styled_header(\"Spotify Song Recommendations (using Spotify ML Engine)\")\n",
    "        display(HTML(song_recs_header))\n",
    "        for track in recommendations['tracks']:\n",
    "            song_recs_line = styled_paragraph(f\"Recommended Track: {track['artists'][0]['name']} - {track['name']}\")\n",
    "            display(HTML(song_recs_line))\n",
    "        \n",
    "        display(Markdown('---'))  # Display horizontal line\n",
    "        feature_plot_header = styled_header(\"Track Features\")\n",
    "        display(HTML(feature_plot_header))        \n",
    "        plot_track_spider_data(artist_name, track_name)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Danceability: describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\")\n",
    "        print(\"\")\n",
    "        print(\"Acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\")\n",
    "        print(\"\")\n",
    "        print(\"Valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\")\n",
    "        print(\"\")\n",
    "        print(\"Speechiness: Detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\")\n",
    "        print(\"\")\n",
    "        print(\"Loudness: The overall loudness of a track in decibels (dB * 10). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between 0 and 60 db.\")\n",
    "        print(\"\")\n",
    "        print(\"Liveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\")\n",
    "        print(\"\")\n",
    "        print(\"Instrumentalness: Predicts whether a track contains no vocals. 'Ooh' and 'aah' sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly 'vocal'. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\")\n",
    "        print(\"\")\n",
    "        print(\"Energy: A measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\")\n",
    "        print(\"\")\n",
    "        print(\"Popularity: The popularity of the track. The value will be between 0 and 100, with 100 being the most popular.\")\n",
    "        print(\"\")\n",
    "        print(\"Key: The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C-sharp/D-flat, 2 = D, and so on. If no key was detected, the value is -1.\")\n",
    "        print(\"\")\n",
    "        print(\"Tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\")\n",
    "        print(\"\")\n",
    "        print(\"Mode: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\")\n",
    "        print(\"\")\n",
    "        print(\"Time Signature: An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of '3/4', to '7/4'.\")\n",
    "        \n",
    "# Attach button click event handler\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "#info = Markdown(\"\"\"# SPOTIFY DECADE PREDICTOR\"\"\")\n",
    "makeDisplay = widgets.VBox([track_name_widget, artist_name_widget,  button, output])\n",
    "display(makeDisplay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
